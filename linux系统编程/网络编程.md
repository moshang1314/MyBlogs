# 1 网络字节序转换函数

## 1.1 htonl()和htons()

```c
#include <stdio.h>
#include <arpa/inet.h>

int main(int argc, int *argv[])
{
	char buf[4] = {192, 168, 1, 2};
	int num = *(int*)buf;
	int sum = htonl(num);

	unsigned char *p = (unsigned char *)&sum;

	printf("%d %d %d %d\n", p[0], p[1], p[2], p[3]);
	
	unsigned short a = 0x0102;
	unsigned short b = htons(a);
	printf("%x\n", b);
	return 0;
}

```

## 1.2 ntohl()和ntohs()

```c
#include <stdio.h>
#include <arpa/inet.h>

int main(int argc, char *argv[])
{
	unsigned char buf[4] = {1, 1, 168, 192};
	int num = *(int*)buf;
	int sum = ntohl(num);
	unsigned char *p = (unsigned char *)&sum;
	printf("%d %d %d %d\n", p[0], p[1], p[2], p[3]);

	return 0;
}
```

# 2 IP地址转换函数

## 2.1 inet_pton()函数

```c
#include <arpa/inet.h>

int inet_pton(int af, const char* src, void*dst);
/*
功能：
	将点分十进制串转换为大端字节序
参数：
	af:
		AF_INET	IPV4
		AF_INET6	IPV6
	src：点分十进制IP首地址
	dst：32位网络字节序数据地址
返回值：
	成功：返回1
*/
```

## 2.2 inet_ntop()函数

```c
#include <arpa/inet.h>

const char* inet_ntop(int af, const void *src, char *dst, socklen_t size);

/*
功能：
	将32位大端的网络IP数据转成点分十进制串
参数：
	af:
		AF_INET	IPV4
		AF_INET	IPV6
	src：32位大端网络数据地址
	dst：存储点分十进制串地址
	size：存储点分十进制的数组大小，16即可，防止溢出
返回值：
	存储点十进制的地址
*/
```

【例】

```c
#include <arpa/inet.h>
#include <stdio.h>

int main(int argc, char *argv[])
{
	char buf[] = "192.168.1.4";
	unsigned int num = 0;
	inet_pton(AF_INET, buf, &num);
	unsigned char *p = (unsigned char *)&num;
	printf("%d %d %d %d\n", p[0], p[1], p[2], p[3]);

	char ip[16]="";
	inet_ntop(AF_INET, &num, ip, 16);
	printf("%s\n", ip);
	return 0;
}
```

# 3 套接字结构体

## 3.1 ipv4套接字结构体

```c
struct sockaddr_in
{
	sa_family_t	sin_family;	/*address family: AF_INET*/
	in_port_t sin_port;	/*port in network byte order*/
	struct in_addr sin_addr; /*internet address*/
}

struct in_addr
{
	uint32_t s_addr; /*address in network byte order*/
}
```

## 3.2 ipv6套接字结构体

```c
struct sockaddr_in6 {
    unsigned short int sin6_family;        /* AF_INET6 */
    __be16 sin6_port;                  /* Transport layer port # */
    __be32 sin6_flowinfo;              /* IPv6 flow information */
    struct in6_addr sin6_addr;         /* IPv6 address */
    __u32 sin6_scope_id;               /* scope id (new in RFC2553) */
};

struct in6_addr {
    union {
        __u8 u6_addr8[16];
        __be16 u6_addr16[8];
        __be32 u6_addr32[4];
    } in6_u;
    #define s6_addr        in6_u.u6_addr8
    #define s6_addr16      in6_u.u6_addr16
    #define s6_addr32     in6_u.u6_addr32
};
 
#define UNIX_PATH_MAX 108
    struct sockaddr_un {
    __kernel_sa_family_t sun_family;   /* AF_UNIX */
    char sun_path[UNIX_PATH_MAX]; /* pathname */
};
```

## 3.3 通用套接字结构体

```c
struct sockaddr {
    sa_family_t sa_family;     /* address family, AF_xxx */
    char sa_data[14];          /* 14 bytes of protocol address */
};
```

# 4 tcp服务通信流程

![](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/Image%20%5B9%5D.png)

## 4.1 创建套接字API

```c
#include <sys/socket.h>

int socket(int domain, int type, int protocol);

/*
参数：
	domain: AF_INET
	type: SOCK_STREAM 	流式套接字 用于tcp通信
	protocol：0
返回值：
	成功：返回文件描述符
	失败：-1
*/
```

## 4.2 连接服务器API

```c
#include <sys/socket.h>

int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

/*
功能：连接服务器
参数：
	sockfd：socket套接字文件描述符
	addr：ipv4套接字结构体的地址
	addrlen：套接字结构体长度
*/
```

```c
#include <stdio.h>
#include <sys/unistd.h>
#include <arpa/inet.h>
#include <sys/socket.h>

int main(int argc, char *argv[])
{
	//创建套接字
	int sock_fd;
	sock_fd = socket(AF_INET, SOCK_STREAM, 0);
	//连接服务器
	struct sockaddr_in addr;
	addr.sin_family = AF_INET;
	addr.sin_port = htons(8080);
	inet_pton(AF_INET, "10.16.14.232", &addr.sin_addr.s_addr);
	connect(sock_fd, (struct sockaddr*)&addr, sizeof(addr));

	//读写数据
	char buf[1024] = "";
	while(1)
	{
		int n = read(STDIN_FILENO, buf, sizeof(buf)); //从标准输入读取数据
		printf("%d\n", n);
		write(sock_fd, buf, n);//发送数据给服务器
		n = read(sock_fd, buf, sizeof(buf));
		write(STDOUT_FILENO, buf, n);
	}

	//关闭描述符
	close(sock_fd);
	return 0;
}
```

## 4.3 bind()函数给套接字绑定固定的端口和ip

```c
#include <sys/socket.h>
int bind(int socketfd, const struct sockaddr *addr, socklen_t addrlen);

/*
参数：
	sockfd：套接字
	addr：ipv4套接字结构体地址
	addrlen: ipv4套接字结构体的大小
返回值：
	成功：0
	失败：-1
*/
```

## 4.4 listen()函数监听套接字

```c
#include <sys/socket.h>
int listen(int sockfd, int backlog);

/*
功能：
当socket函数创建一个套接字时，它被假设为一个主动套接字，也就是说，它是一个将调用connect连接的客户套接字。listen函数把一个未连接的套接字转换成一个被动套接字，指示内核应该接受指向该套接字的连接请求。如下图所示，调用listen导致套接字从CLOSED状态转换到LISTEN状态。
参数：
	sockfd：套接字
	backlog：已完成连接队列和未完成连接队列数之和的最大值， 一般为128
*/
```

## 4.5 accept()函数从已完成队列提取新的连接

```c
#include <sys/socket.h>
int accept(int socket, struct sockaddr *restrict address, socklen_t *restrict address_len);

/*
功能：从已完成连接队列里提取连接
参数：
	socket: 套接字
	address：获取的客户端的ip和端口信息 ipv4套接字结构体地址
	address_len：ipv4套接字结构体的大小的地址
返回值：新的已连接套接字的文件描述符
【注】：如果连接队列没有新的连接，accept会阻塞
*/
```

## 4.6  tcp服务器通信流程

1) 创建套接字 socket
2) 绑定ip和端口 bind
3) 监听套接字 listen
4) 提取连接 accept
5) 读写
6) 关闭

> nc 10.16.14.232 8080	#终端模拟客户端与服务器通信

【例】

```c
#include <stdio.h>
#include <arpa/inet.h>
#include <sys/socket.h>
#include <unistd.h>
#include <stdlib.h>
#include <string.h>

int main(int argc, char *argv[])
{
	//创建套接字
	int lfd = socket(AF_INET, SOCK_STREAM, 0);
	//绑定
	struct sockaddr_in addr;
	addr.sin_family = AF_INET;
	addr.sin_port = htons(8000);
	//addr.sin_addr.s_addr = INADDR_ANY; //该宏值为0，表示绑定的是通配地址，即本机所有ip均与之绑定
	inet_pton(AF_INET, "10.0.0.204", &addr.sin_addr.s_addr);
	int ret = bind(lfd, (struct sockaddr *)&addr, sizeof(addr));
	if(ret < 0)
	{
		perror("bind");
		exit(0);
	}
	//监听
	listen(lfd, 128);
	puts("already listen!");
	//提取
	struct sockaddr_in cliaddr;
	socklen_t len = sizeof(cliaddr);
	int cfd = accept(lfd, (struct sockaddr *)&cliaddr, &len); 
	char ip[16] = "";
	printf("a new client ip=%s port=%d\n", inet_ntop(AF_INET, &cliaddr.sin_addr.s_addr, ip, 16), ntohs(cliaddr.sin_port));
	//读写
	while(1)
	{
		char buf[1024] = "";
		bzero(buf, sizeof(buf));
		int n = read(STDIN_FILENO, buf, sizeof(buf));
		write(cfd, buf, n);
		read(cfd, buf, sizeof(buf));
		printf("%s\n", buf);
	}
	//关闭
	close(lfd);
	close(cfd);

	return 0;
}
```

```c
#include <string.h>
void *bzero(void *s, int n);

/*
功能：
	将内存块的前n个字节清零
*/
```



## 4.7 对服务器端API的封装

wrap.h

```c
#ifndef __WRAP_H_
#define __WRAP_H_
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <strings.h>

void perr_exit(const char *s);
int Accept(int fd, struct sockaddr *sa, socklen_t *salenptr);
int Bind(int fd, const struct sockaddr *sa, socklen_t salen);
int Connect(int fd, const struct sockaddr *sa, socklen_t salen);
int Listen(int fd, int backlog);
int Socket(int family, int type, int protocol);
ssize_t Read(int fd, void *ptr, size_t nbytes);
ssize_t Write(int fd, const void *ptr, size_t nbytes);
int Close(int fd);
ssize_t Readn(int fd, void *vptr, size_t n);
ssize_t Writen(int fd, const void *vptr, size_t n);
ssize_t my_read(int fd, char *ptr);
ssize_t Readline(int fd, void *vptr, size_t maxlen);
int tcp4bind(short port,const char *IP);
#endif
```

wrap.c

```c
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <strings.h>

void perr_exit(const char *s)
{
	perror(s);
	exit(-1);
}

int Accept(int fd, struct sockaddr *sa, socklen_t *salenptr)
{
	int n;

again:
	if ((n = accept(fd, sa, salenptr)) < 0) {
		if ((errno == ECONNABORTED) || (errno == EINTR))//如果是被信号中断和软件层次中断,不能退出
			goto again;
		else
			perr_exit("accept error");
	}
	return n;
}

int Bind(int fd, const struct sockaddr *sa, socklen_t salen)
{
    int n;

	if ((n = bind(fd, sa, salen)) < 0)
		perr_exit("bind error");

    return n;
}

int Connect(int fd, const struct sockaddr *sa, socklen_t salen)
{
    int n;

	if ((n = connect(fd, sa, salen)) < 0)
		perr_exit("connect error");

    return n;
}

int Listen(int fd, int backlog)
{
    int n;

	if ((n = listen(fd, backlog)) < 0)
		perr_exit("listen error");

    return n;
}

int Socket(int family, int type, int protocol)
{
	int n;

	if ((n = socket(family, type, protocol)) < 0)
		perr_exit("socket error");

	return n;
}

ssize_t Read(int fd, void *ptr, size_t nbytes)
{
	ssize_t n;

again:
	if ( (n = read(fd, ptr, nbytes)) == -1) {
		if (errno == EINTR)//如果是被信号中断,不应该退出
			goto again;
		else
			return -1;
	}
	return n;
}

ssize_t Write(int fd, const void *ptr, size_t nbytes)
{
	ssize_t n;

again:
	if ( (n = write(fd, ptr, nbytes)) == -1) {
		if (errno == EINTR)
			goto again;
		else
			return -1;
	}
	return n;
}

int Close(int fd)
{
    int n;
	if ((n = close(fd)) == -1)
		perr_exit("close error");

    return n;
}

/*参三: 应该读取固定的字节数数据*/
ssize_t Readn(int fd, void *vptr, size_t n)
{
	size_t  nleft;              //usigned int 剩余未读取的字节数
	ssize_t nread;              //int 实际读到的字节数
	char   *ptr;

	ptr = vptr;
	nleft = n;

	while (nleft > 0) {
		if ((nread = read(fd, ptr, nleft)) < 0) {
			if (errno == EINTR)
				nread = 0;
			else
				return -1;
		} else if (nread == 0)
			break;

		nleft -= nread;
		ptr += nread;
	}
	return n - nleft;
}
/*:固定的字节数数据*/
ssize_t Writen(int fd, const void *vptr, size_t n)
{
	size_t nleft;
	ssize_t nwritten;
	const char *ptr;

	ptr = vptr;
	nleft = n;
	while (nleft > 0) {
		if ( (nwritten = write(fd, ptr, nleft)) <= 0) {
			if (nwritten < 0 && errno == EINTR)
				nwritten = 0;
			else
				return -1;
		}

		nleft -= nwritten;
		ptr += nwritten;
	}
	return n;
}

static ssize_t my_read(int fd, char *ptr)
{
	static int read_cnt;
	static char *read_ptr;
	static char read_buf[100];

	if (read_cnt <= 0) {
again:
		if ( (read_cnt = read(fd, read_buf, sizeof(read_buf))) < 0) {
			if (errno == EINTR)
				goto again;
			return -1;
		} else if (read_cnt == 0)
			return 0;
		read_ptr = read_buf;
	}
	read_cnt--;
	*ptr = *read_ptr++;

	return 1;
}

ssize_t Readline(int fd, void *vptr, size_t maxlen)
{
	ssize_t n, rc;
	char    c, *ptr;

	ptr = vptr;
	for (n = 1; n < maxlen; n++) {
		if ( (rc = my_read(fd, &c)) == 1) {
			*ptr++ = c;
			if (c  == '\n')
				break;
		} else if (rc == 0) {
			*ptr = 0;
			return n - 1;
		} else
			return -1;
	}
	*ptr  = 0;

	return n;
}

int tcp4bind(short port,const char *IP)
{
    struct sockaddr_in serv_addr;
    int lfd = Socket(AF_INET,SOCK_STREAM,0);
    bzero(&serv_addr,sizeof(serv_addr));
    if(IP == NULL){
        //如果这样使用 0.0.0.0,任意ip将可以连接
        serv_addr.sin_addr.s_addr = INADDR_ANY;
    }else{
        if(inet_pton(AF_INET,IP,&serv_addr.sin_addr.s_addr) <= 0){
            perror(IP);//转换失败
            exit(1);
        }
    }
    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port   = htons(port);
   // int opt = 1;
	//setsockopt(lfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    Bind(lfd,(struct sockaddr *)&serv_addr,sizeof(serv_addr));
    return lfd;
}
```

【注】**当read()读取返回为0时，表示对方关闭了连接。**

**【注】**当程序在执行处于阻塞状态的系统调用时接收到信号，并且我们为该信号设置了信号处理函数，在信号处理函数返回后，程序将面临继续执行或不执行慢速系统调用两种选择，默认情况下是系统调用将被中断，并且errno被设置为EINTR。我们可以选择继续执行，有以下两种方法：

1.在设置信号处理函数的时候，为信号设置SA_RESTART标志以自动重启被该信号中断的系统调用，但是该方法对某些慢速系统调用无效，比如epoll_wait,poll,seletc等慢速系统调用，即使给信号设置了该选项，也会被中断。具体对那些慢速系统调用无效可查看manpages。

2.**手动重启，对慢速系统调用的返回值进行判断，若errno==EINTR,则重新调用慢速系统调用**


综上所述，若想要重启系统调用，最好使用方法2。

## 4.8 TCP三次握手建立连接

![](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/image-20220905213135777.png)

![img](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/wps2.jpg)

**建立连接（三次握手）的过程：**

1. 客户端发送一个带SYN标志的TCP报文到服务器。这是三次握手过程中的段1。

> 客户端发出段1，SYN位表示连接请求。序号是1000，这个序号在网络通讯中用作临时的地址，每发一个数据字节，这个序号要加1，这样在接收端可以根据序号排出数据包的正确顺序，也可以发现丢包的情况，另外，规定SYN位和FIN位也要占一个序号，这次虽然没发数据，但是由于发了SYN位，因此下次再发送应该用序号1001。**mss表示最大段尺寸，如果一个段太大，封装成帧后超过了链路层的最大帧长度，就必须在IP层分片，为了避免这种情况，客户端声明自己的最大段尺寸，建议服务器端发来的段不要超过这个长度。**

2. 服务器端回应客户端，是三次握手中的第2个报文段，同时带ACK标志和SYN标志。它表示对刚才客户端SYN的回应；同时又发送SYN给客户端，询问客户端是否准备好进行数据通讯。

   > 服务器发出段2，也带有SYN位，同时置ACK位表示确认，确认序号是1001，表示“我接收到序号1000及其以前所有的段，请你下次发送序号为1001的段”，也就是应答了客户端的连接请求，同时也给客户端发出一个连接请求，同时声明最大尺寸为1024。

   

3) 客户必须再次回应服务器端一个ACK报文，这是报文段3。

> 客户端发出段3，对服务器的连接请求进行应答，确认序号是8001。在这个过程中，客户端和服务器分别给对方发了连接请求，也应答了对方的连接请求，其中服务器的请求和应答在一个段中发出，因此一共有三个段用于建立连接，称为“三方握手（three-way-handshake）”。在建立连接的同时，双方协商了一些信息，例如双方发送序号的初始值、最大段尺寸等。

**在TCP通讯中，如果一方收到另一方发来的段，读出其中的目的端口号，发现本机并没有任何进程使用这个端口，就会应答一个包含RST位的段给另一方。例如，服务器并没有任何进程使用8080端口，我们却用telnet客户端去连接它，服务器收到客户端发来的SYN段就会应答一个RST段，客户端的telnet程序收到RST段后报告错误Connection refused：**

> $ telnet 192.168.0.200 8080
>
> Trying 192.168.0.200...
>
> telnet: Unable to connect to remote host: Connection refused

**数据传输的过程：**

1. 客户端发出段4，包含从序号1001开始的20个字节数据。

2. 服务器发出段5，确认序号为1021，对序号为1001-1020的数据表示确认收到，同时请求发送序号1021开始的数据，服务器在应答的同时也向客户端发送从序号8001开始的10个字节数据，这称为piggyback。

3. 客户端发出段6，对服务器发来的序号为8001-8010的数据表示确认收到，请求发送序号8011开始的数据。

> 在数据传输过程中，ACK和确认序号是非常重要的，应用程序交给TCP协议发送的数据会暂存在TCP层的发送缓冲区中，发出数据包给对方之后，只有收到对方应答的ACK段才知道该数据包确实发到了对方，可以从发送缓冲区中释放掉了，如果因为网络故障丢失了数据包或者丢失了对方发回的ACK段，经过等待超时后TCP协议自动将发送缓冲区中的数据包重发。

## 4.9 TCP四次挥手断开连接

![](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/image-20220905214759064.png)

**关闭连接（四次握手）的过程：**

由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。

1. 客户端发出段7，FIN位表示关闭连接的请求。

2. 服务器发出段8，应答客户端的关闭连接请求。

3. 服务器发出段9，其中也包含FIN位，向客户端发送关闭连接请求。

4. 客户端发出段10，应答服务器的关闭连接请求。

建立连接的过程是三方握手，而关闭连接通常需要4个段，服务器的应答和关闭连接请求通常不合并在一个段中，因为有连接半关闭的情况，这种情况下客户端关闭连接之后就不能再发送数据给服务器了，但是服务器还可以发送数据给客户端，直到服务器也关闭连接为止。

# 5 TCP滑动窗口（流量控制）

![img](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/wps3.jpg)

1. 发送端发起连接，声明最大段尺寸是1460，初始序号是0，窗口大小是4K，表示“我的接收缓冲区还有4K字节空闲，你发的数据不要超过4K”。接收端应答连接请求，声明最大段尺寸是1024，初始序号是8000，窗口大小是6K。发送端应答，三方握手结束。

2. 发送端发出段4-9，每个段带1K的数据，发送端根据窗口大小知道接收端的缓冲区满了，因此停止发送数据。

3. 接收端的应用程序提走2K数据，接收缓冲区又有了2K空闲，接收端发出段10，在应答已收到6K数据的同时声明窗口大小为2K。

4. 接收端的应用程序又提走2K数据，接收缓冲区有4K空闲，接收端发出段11，重新声明窗口大小为4K。

5. 发送端发出段12-13，每个段带2K数据，段13同时还包含FIN位。

6. 接收端应答接收到的2K数据（6145-8192），再加上FIN位占一个序号8193，因此应答序号是8194，连接处于半关闭状态，接收端同时声明窗口大小为2K。

7. 接收端的应用程序提走2K数据，接收端重新声明窗口大小为4K。

8. 接收端的应用程序提走剩下的2K数据，接收缓冲区全空，接收端重新声明窗口大小为6K。

9. 接收端的应用程序在提走全部数据后，决定关闭连接，发出段17包含FIN位，发送端应答，连接完全关闭。

上图在接收端用小方块表示1K数据，实心的小方块表示已接收到的数据，虚线框表示接收缓冲区，因此套在虚线框中的空心小方块表示窗口大小，从图中可以看出，随着应用程序提走数据，虚线框是向右滑动的，因此称为滑动窗口。

从这个例子还可以看出，发送端是一K一K地发送数据，而接收端的应用程序可以两K两K地提走数据，当然也有可能一次提走3K或6K数据，或者一次只提走几个字节的数据。也就是说，应用程序所看到的数据是一个整体，或说是一个流（stream），在底层通讯中这些数据可能被拆成很多数据包来发送，但是一个数据包有多少字节对应用程序是不可见的，因此**TCP协议是面向流的协议。**而**UDP是面向消息的协议，每个UDP段都是一条消息，应用程序必须以消息为单位提取数据，不能一次提取任意字节的数据，这一点和TCP是很不同的。**

# 6 多进程实现并发服务器流程

```c
#include <stdio.h>
#include <sys/socket.h>
#include <unistd.h>
#include <signal.h>
#include <sys/wait.h>
#include "wrap.h"
void free_process(int sig)
{
	pid_t pid;
	while(1)
	{
		pid = waitpid(-1,NULL,WNOHANG);
		if(pid <=0 )//小于0 子进程全部退出了 =0没有进程没有退出
		{
			break;
		}
		else
		{
			printf("child pid =%d\n",pid);
		}
	}



}
int main(int argc, char *argv[])
{
	sigset_t set;
	sigemptyset(&set);
	sigaddset(&set,SIGCHLD);
	sigprocmask(SIG_BLOCK,&set,NULL);
	//创建套接字,绑定
	int lfd = tcp4bind(8008,NULL);
	//监
	Listen(lfd,128);
	//提取
	//回射
	struct sockaddr_in cliaddr;
	socklen_t len = sizeof(cliaddr);
	while(1)
	{
		char ip[16]="";
		//提取连接,
		int cfd = Accept(lfd,(struct sockaddr *)&cliaddr,&len);
		printf("new client ip=%s port=%d\n",inet_ntop(AF_INET,&cliaddr.sin_addr.s_addr,ip,16),
				ntohs(cliaddr.sin_port));
		//fork创建子进程
		pid_t pid;
		pid = fork();
		if(pid < 0)
		{
			perror("");
			exit(0);
		}
		else if(pid == 0)//子进程
		{
			//关闭lfd
			close(lfd);
			while(1)
			{
			char buf[1024]="";

			int n = read(cfd,buf,sizeof(buf));
			if(n < 0)
			{
				perror("");
				close(cfd);
				exit(0);
			}
			else if(n == 0)//对方关闭j
			{
				printf("client close\n");
				close(cfd);
				exit(0);
			
			}
			else
			{
				printf("%s\n",buf);
				write(cfd,buf,n);
			//	exit(0);	
			}
			}
		
		}
		else//父进程
		{
			close(cfd);
			//回收
			//注册信号回调
			struct sigaction act;
			act.sa_flags =0;
			act.sa_handler = free_process;
			sigemptyset(&act.sa_mask);
			sigaction(SIGCHLD,&act,NULL);
			sigprocmask(SIG_UNBLOCK,&set,NULL);
		
		}
	}
	//关闭



	return 0;
}
```

# 7 多线程实现并发服务器流程

```c
#include <stdio.h>
#include <pthread.h>
#include "wrap.h"

typedef struct c_info
{
	int cfd;
	struct sockaddr_in cliaddr;
}CINFO;

void *client_fun(void *arg);

int main(int argc, char *argv[])
{
	if(argc < 2)
	{
		printf("argc < 2???  \n ./a.out 8000  \n");
		return 0;
	}
	pthread_attr_t attr;
	pthread_attr_init(&attr);
	pthread_attr_setdetachstate(&attr, PTHTEAD_CREATE_DETACHED);
	short port = atoi(argv[1]);
	int lfd = tcp4bind(port, NULL);//创建套件字，并绑定端口和ip
	Listen(lfd, 128);
	struct sockaddr_in cliaddr;
	socklen_t len = sizeof(cliaddr);
	CINFO *info;
	while(1)
	{
		int cfd = Accept(lfd, (struct sockaddr *)&cliaddr, &len);
		pthread_t tid;
		info = malloc(sizeof(CINFO));
		info->cfd = cfd;
		info->cliaddr = cliaddr;
		pthread_create(&tid, &attr, client_fun, info);
	}
	return 0;
}

void* client_fun(void *arg)
{
	CINFO *info = (CINFO *)arg;
	char ip[16] = "";
	printf("new client ip=%s port=%d\n", inet_ntop(AF_INET, &(info->cliaddr.sin_addr.s_addr), ip, 16), ntohs(info->cliaddr.sin_port));
	while(1)
	{
		char buf[1024]="";
		int count = 0;
		count = read(info->cfd, buf, sizeof(buf));
		if(count < 0)
		{
			perror("read");
			break;
		}
		else if(count == 0)
		{
			printf("client close\n");
			break;
		}
		else
		{
			printf("%s\n", buf);
			write(info->cfd, buf, count);
		}
	}
	close(info->cfd);
	free(info);
	info = NULL;
}
```

# 8 TCP状态转换

![](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/70)

![](https://cdn.jsdelivr.net/gh/moshang1314/myBlog@main/image/71)

> **netstat命令**是一个监控TCP/IP网络的非常有用的工具，他可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。
>
> -a：显示所有连线中的socket
>
> -c或–continuous：持续列出网络状态
>
> -C或–cache：显示路由器配置的快取信息
>
> -n或–numeric：直接使用ip地址，而不通过域名服务器
>
> -r或–route：显示路由表
>
> -t或–tcp：显示TCP传输协议的连线状况
>
> -p或–program：显示正在使用Socket的程序识别码和程序名称

# 9 半关闭状态

> 当客户端发送FIN并收到ACK后进入FIN_WAIT2状态，此时关闭了写缓冲区，还可以从读缓冲区读取数据，即接收来自服务端的数据。当收到服务端的FIN之后才会完全关闭。

```c
#include <sys/socket.h>

int shutdown(int sockfd, int how);
/*
sockfd：需要关闭的socket描述符
how：关闭状态：
	SHUT_RD(0)：关闭sockfd上的读功能，此选项不允许sockfd进行读操作。
	该套接字不再接收数据。任何在当前套接字接受的数据都会被无声的丢弃掉。
	SHUT_WR(1)：关闭sockfd的写功能，此选项将不允许sockfd进行写操作。进程不能对此套接字发出写操作。
	SHUT_RDWR(2)：关闭sockfd的读写功能。
*/
```

【注意】

使**用close中止一个连接，但它只是减少描述符的引用计数，并不直接关闭连接，只有当描述符的引用计数为0时才关闭连接。**

**shutdown不考虑描述符的引用计数，直接关闭描述符**。也可选择中止一个方向的连接，只中止读或只中止写。

注意:

1. 如果有多个进程共享一个套接字，close每被调用一次，计数减1，直到计数为0时，也就是所用进程都调用了close，套接字将被释放。 

2. **在多进程中如果一个进程调用了shutdown(sfd, SHUT_RDWR)后，其它的进程将无法进行通信。但，如果一个进程close(sfd)将不会影响到其它进程。**

# 10 2MSL

2MSL (Maximum Segment Lifetime) TIME_WAIT状态的存在有两个理由：

（1）**让4次握手关闭流程更加可靠**；4次握手的最后一个ACK是是由主动关闭方发送出去的，若这个ACK丢失，被动关闭方会再次发一个FIN过来。若主动关闭方能够保持一个2MSL的TIME_WAIT状态，则有更大的机会让丢失的ACK被再次发送出去。

（2）防止lost duplicate对后续新建正常链接的传输造成破坏。lost duplicate在实际的网络中非常常见，经常是由于路由器产生故障，路径无法收敛，导致一个packet在路由器A，B，C之间做类似死循环的跳转。IP头部有个TTL，限制了一个包在网络中的最大跳数，因此这个包有两种命运，要么最后TTL变为0，在网络中消失；要么TTL在变为0之前路由器路径收敛，它凭借剩余的TTL跳数终于到达目的地。但非常可惜的是TCP通过超时重传机制在早些时候发送了一个跟它一模一样的包，并先于它达到了目的地，因此它的命运也就注定被TCP协议栈抛弃。

另外一个概念叫做incarnation connection，指跟上次的socket pair一摸一样的新连接，叫做incarnation of previous connection。lost uplicate加上incarnation connection，则会对我们的传输造成致命的错误。

TCP是流式的，所有包到达的顺序是不一致的，依靠序列号由TCP协议栈做顺序的拼接；假设一个incarnation connection这时收到的seq=1000, 来了一个lost duplicate为seq=1000，len=1000, 则TCP认为这个lost duplicate合法，并存放入了receive buffer，导致传输出现错误。通过一个2MSL TIME_WAIT状态，确保所有的lost duplicate都会消失掉，避免对新连接造成错误。

该状态为什么设计在**主动关闭这一方**：

（1）发最后ACK的是主动关闭一方。

（2）只要有一方保持TIME_WAIT状态，就能起到避免incarnation connection在2MSL内的重新建立，不需要两方都有。

如何正确对待2MSL TIME_WAIT?

RFC要求socket pair在处于TIME_WAIT时，不能再起一个incarnation connection。但绝大部分TCP实现，强加了更为严格的限制。在2MSL等待期间，socket中使用的本地端口在默认情况下不能再被使用。

若A 10.234.5.5 : 1234和B 10.55.55.60 : 6666建立了连接，A主动关闭，那么在A端只要port为1234，无论对方的port和ip是什么，都不允许再起服务。这甚至比RFC限制更为严格，RFC仅仅是要求socket pair不一致，而实现当中只要这个port处于TIME_WAIT，就不允许起连接。这个限制对主动打开方来说是无所谓的，因为一般用的是临时端口；但对于被动打开方，一般是server，就悲剧了，因为server一般是熟知端口。比如http，一般端口是80，不可能允许这个服务在2MSL内不能起来。

解决方案是给服务器的socket设置SO_REUSEADDR选项，这样的话就算熟知端口处于TIME_WAIT状态，在这个端口上依旧可以将服务启动。当然，虽然有了SO_REUSEADDR选项，但sockt pair这个限制依旧存在。比如上面的例子，A通过SO_REUSEADDR选项依旧在1234端口上起了监听，但这时我们若是从B通过6666端口去连它，TCP协议会告诉我们连接失败，原因为Address already in use.

**RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。**

RFC (Request For Comments)，是一系列以编号排定的文件。收集了有关因特网相关资讯，以及UNIX和因特网社群的[软件](http://baike.baidu.com/view/37.htm)文件。

## 10.1 程序设计中的问题

做一个测试，首先启动server，然后启动client，用Ctrl-C终止server，马上再运行server，运行结果：

> itcast$ ./server
>
> bind error: Address already in use 

这是因为，虽然server的应用程序终止了，但TCP协议层的连接并没有完全断开，因此不能再次监听同样的server端口。我们用netstat命令查看一下：

> itcast$ netstat -apn |grep 6666
>
> tcp     1    0 192.168.1.11:38103    192.168.1.11:6666    CLOSE_WAIT  3525/client   
>
> tcp     0    0 192.168.1.11:6666    192.168.1.11:38103    FIN_WAIT2  -      

server终止时，socket描述符会自动关闭并发FIN段给client，**client收到FIN后处于CLOSE_WAIT状态**，但是client并没有终止，也没有关闭socket描述符，因此不会发FIN给server，因此server的TCP连接处于FIN_WAIT2状态。

现在用Ctrl-C把client也终止掉，再观察现象：

> itcast$ netstat -apn |grep 6666
>
> tcp     0    0 192.168.1.11:6666    192.168.1.11:38104    TIME_WAIT  -
>
> itcast$ ./server
>
> bind error: Address already in use

**client终止时自动关闭socket描述符，server的TCP连接收到client发的FIN段后处于TIME_WAIT状态。**TCP协议规定，**主动关闭连接的一方要处于TIME_WAIT状态**，等待两个MSL（maximum segment lifetime）的时间后才能回到CLOSED状态，因为我们先Ctrl-C终止了server，所以server是主动关闭连接的一方，在TIME_WAIT期间仍然不能再次监听同样的server端口。

**MSL在RFC 1122中规定为两分钟，**但是各操作系统的实现不同，在Linux上一般经过半分钟后就可以再次启动server了。至于为什么要规定TIME_WAIT的时间，可参考UNP 2.7节。

# 11 TCP异常断开

## 11.1 心跳检测机制

在TCP网络通信中，经常会出现客户端和服务器之间的非正常断开，需要实时检测查询链接状态。常用的解决方法就是在程序中加入心跳机制。

Heart-Beat线程

这个是最常用的简单方法。在接收和发送数据时个人设计一个守护进程(线程)，定时发送Heart-Beat包，客户端/服务器收到该小包后，立刻返回相应的包即可检测对方是否实时在线。

该方法的好处是通用，但缺点就是会改变现有的通讯协议！大家一般都是使用业务层心跳来处理，主要是灵活可控。

UNIX网络编程不推荐使用SO_KEEPALIVE来做心跳检测，还是在业务层以心跳包做检测比较好，也方便控制。

## 11.2 设置TCP属性

SO_KEEPALIVE 保持连接检测对方主机是否崩溃，避免（服务器）永远阻塞于TCP连接的输入。设置该选项后，如果2小时内在此套接口的任一方向都没有数据交换，TCP就自动给对方发一个保持存活探测分节(keepalive probe)。这是一个对方必须响应的TCP分节.它会导致以下三种情况：对方接收一切正常：以期望的ACK响应。**2小时后，TCP将发出另一个探测分节。**对方已崩溃且已重新启动：以RST响应。套接口的待处理错误被置为ECONNRESET，套接 口本身则被关闭。对方无任何响应：源自berkeley的TCP发送另外8个探测分节，相隔75秒一个，试图得到一个响应。在发出第一个探测分节11分钟 15秒后若仍无响应就放弃。套接口的待处理错误被置为ETIMEOUT，套接口本身则被关闭。如ICMP错误是“host unreachable(主机不可达)”，说明对方主机并没有崩溃，但是不可达，这种情况下待处理错误被置为EHOSTUNREACH。

根据上面的介绍我们可以知道对端以一种非优雅的方式断开连接的时候，我们可以设置SO_KEEPALIVE属性使得我们在2小时以后发现对方的TCP连接是否依然存在。

> keepAlive = 1;
>
> setsockopt(listenfd, SOL_SOCKET, SO_KEEPALIVE, (void*)&keepAlive, sizeof(keepAlive));

如果我们不能接受如此之长的等待时间，从TCP-Keepalive-HOWTO上可以知道一共有两种方式可以设置，一种是修改内核关于网络方面的 配置参数，另外一种就是SOL_TCP字段的TCP_KEEPIDLE， TCP_KEEPINTVL， TCP_KEEPCNT三个选项。

1. The tcp_keepidle parameter specifies the interval of inactivity that causes TCP to generate a KEEPALIVE transmission for an application that requests them. tcp_keepidle defaults to 14400 (two hours). 

/*开始首次KeepAlive探测前的TCP空闭时间 */

2. The tcp_keepintvl parameter specifies the interval between the nine retriesthat are attempted if a KEEPALIVE transmission is not acknowledged. tcp_keep ntvldefaults to 150 (75 seconds). 

/* 两次KeepAlive探测间的时间间隔 */

3. The tcp_keepcnt option specifies the maximum number of keepalive probes tobe sent. The value of TCP_KEEPCNT is an integer value between 1 and n, where n s the value of the systemwide tcp_keepcnt parameter. 

/* 判定断开前的KeepAlive探测次数*/

> int keepIdle = 1000;
>
> int keepInterval = 10;
>
> int keepCount = 10;
>
>  
>
> Setsockopt(listenfd, SOL_TCP, TCP_KEEPIDLE, (void *)&keepIdle, sizeof(keepIdle));
>
> Setsockopt(listenfd, SOL_TCP,TCP_KEEPINTVL, (void *)&keepInterval, sizeof(keepInterval));
>
> Setsockopt(listenfd,SOL_TCP, TCP_KEEPCNT, (void *)&keepCount, sizeof(keepCount));

SO_KEEPALIVE设置空闲2小时才发送一个“保持存活探测分节”，不能保证实时检测。对于判断网络断开时间太长，对于需要及时响应的程序不太适应。

当然也可以修改时间间隔参数，但是会影响到所有打开此选项的套接口！关联了完成端口的socket可能会忽略掉该套接字选项。

# 12 端口复用

在server的TCP连接没有完全断开之前不允许重新监听是不合理的。因为，TCP连接没有完全断开指的是connfd（127.0.0.1:6666）没有完全断开，而我们重新监听的是lis-tenfd（0.0.0.0:6666），虽然是占用同一个端口，但IP地址不同，connfd对应的是与某个客户端通讯的一个具体的IP地址，而listenfd对应的是wildcard address。解决这个问题的方法是使用setsockopt()设置socket描述符的选项SO_REUSEADDR为1，表示允许创建端口号相同但IP地址不同的多个socket描述符。

在server代码的socket()和bind()调用之间插入如下代码：

> ​	int opt = 1;
>
> ​	setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

【注意】之前使用这个端口的进程便无法使用了。

# 13 高并发服务器

> 高并发阻塞	消耗资源
>
> 高并发忙轮询	消耗CPU
>
> 多路IO转接（多路IO复用）：**内核监听多个文件描述符的属性（读写缓冲区）变化**，如果某个文件的读缓冲区发生了变化，内核把这个事件告知应用层。
>
> **什么是IO多路复用？**
>
> 举一个简单地网络服务器的例子，如果你的服务器需要和多个客户端保持连接，处理客户端的请求，属于多进程的并发问题，如果创建很多个进程来处理这些IO流，会导致CPU占有率很高。所以人们提出了I/O多路复用模型：**一个线程，通过记录I/O流的状态来同时管理多个I/O。**
> 

## 13.1 select()函数

> select()函数允许程序监视多个文件描述符，等待所监视的一个或者多个文件描述符变为“准备好”的状态。所谓的”准备好“状态是指：文件描述符不再是阻塞状态，可以用于某类IO操作了，包括可读，可写，发生异常三种。
>
> 我们使用select来监视文件描述符时，要向内核传递的信息包括：
>  1、我们要监视的文件描述符个数
>  2、每个文件描述符，我们可以监视它的一种或多种状态，包括：可读，可写，发生异常三种。
>  3、要等待的时间，监视是一个过程，我们希望内核监视多长时间，然后返回给我们监视结果呢？
>  4、监视结果包括：准备好了的文件描述符个数，对于读，写，异常，分别是哪儿个文件描述符准备好了。

```c
#include <sys/select.h>
/* According to earlier standards */
#include <sys/time.h>
#include <sys/types.h>
#include <unistd.h>
int select(int nfds, fd_set *readfds, fd_set *writefds,
			fd_set *exceptfds, struct timeval *timeout);
/*
	nfds: 		监控的文件描述符集里最大文件描述符加1，因为此参数会告诉内核检测前多少个文件描述符的状态
	readfds：	监控有读数据到达文件描述符集合，传入传出参数
	writefds：	监控写数据到达文件描述符集合，传入传出参数
	exceptfds：	监控异常发生达文件描述符集合,如带外数据到达异常，传入传出参数
	timeout：	定时阻塞监控时间，3种情况
				1.NULL，永远等下去
				2.设置timeval，等待固定时间
				3.设置timeval里时间均为0，检查描述字后立即返回，轮询
	返回值：返回的是变化的文件描述符的个数
	【注意】变化的文件描述符会存在在监听的集合中，未变化的文件描述符会被删除
*/
	struct timeval {
		long tv_sec; /* seconds */
		long tv_usec; /* microseconds */
	};
	void FD_CLR(int fd, fd_set *set); 	//把文件描述符集合里fd位清0
	int FD_ISSET(int fd, fd_set *set); 	//测试文件描述符集合里fd是否置1
	void FD_SET(int fd, fd_set *set); 	//把文件描述符集合里fd位置1
	void FD_ZERO(fd_set *set); 			//把文件描述符集合里所有位清0
```

> select函数优缺点：
>
> 优点：
>
> * select的可移植性较好，可以跨平台；
> * select可设置的监听时间timeout精度更好，可精确到微秒，而poll为毫秒。
>
> 缺点：
>
> * select能监听的文件描述符个数受限于FD_SETSIZE，一般为1024，单纯改变进程打开的文件描述符个数并不能改变select监听文件个数。
> * 每次都需要把需要监听的文件描述符集合由用户空间拷贝到内核空间。
>
> * select返回的就绪文件描述符集合，需要用户循环遍历所监听的所有文件描述符是否在该集合中，当监听描述符数量很大时效率较低。

【例】

```c
#include <stdio.h>
#include <sys/select.h>
#include <sys/types.h>
#include <unistd.h>
#include "wrap.h"
#include <sys/time.h>

#define PORT 8888

int main(int argc, char *argv[])
{
	//创建套接字，绑定
	int lfd = tcp4bind(PORT, NULL);
	//监听
	Listen(lfd, 128);
	int maxfd = lfd;//最大的文件描述符
	fd_set oldset, rset;
	FD_ZERO(&oldset);
	FD_ZERO(&rset);
	//将lfd添加到oldset集合中
	FD_SET(lfd, &oldset);
	while(1)
	{
		rset = oldset;
		int n = select(maxfd+1, &rset, NULL, NULL, NULL); 
		if(n < 0)
		{
			perror(" ");
			break;
		}
		else if(n == 0)
		{
			continue;
		}
		else //监听到了文件描述符的变化
		{
			if(FD_ISSET(lfd, &rset))
			{
				struct sockaddr_in cliaddr;
				socklen_t len = sizeof(cliaddr);
				char ip[16] = "";
				//提取新的连接
				int cfd = Accept(lfd, (struct sockaddr*)&cliaddr, &len);
				printf("new client ip=%s port=%d\n", inet_ntop(AF_INET, &cliaddr.sin_addr.s_addr, ip, 16), ntohs(cliaddr.sin_port));
				//将cfd添加至oldset集合中，以便监听
				FD_SET(cfd, &oldset);
				//更新maxfd
				if(cfd > maxfd)
					maxfd = cfd;
				//如果只有lfd变化，continue
				if(--n == 0)
				{
					continue;
				}
			}
			//cfd 遍历lfd之后的文件描述符是否在rset集合中，若在，表明发生了变化
			for(int i = lfd+1; i <= maxfd; i++)
			{
				//如果i文件描述符在rset集合中
				if(FD_ISSET(i, &rset))
				{
					char buf[1500] = "";
					int ret = Read(i, buf, sizeof(buf));
					if(ret < 0)//出错则将cfd关闭，从oldset中删除cfd,从oldset中删除cfd
					{
						perror("read");
						close(i);
						FD_CLR(i, &oldset);
						continue;
					}
					else if(ret == 0)
					{
						printf("client close\n");
						close(i);
						FD_CLR(i, &oldset);
					}
					else
					{
						printf("%s\n", buf);
						Write(i, buf, ret);
					}
				}
			}
		}
	}
}
```

【例2 数组版】

```c
//进阶版select，通过数组防止遍历1024个描述符
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <arpa/inet.h>
#include <ctype.h>

#include "wrap.h"

#define SERV_PORT 8888

int main(int argc, char *argv[])
{
    int i, j, n, maxi;

    int nready, client[FD_SETSIZE];                 /* 自定义数组client, 防止遍历1024个文件描述符  FD_SETSIZE默认为1024 */
    int maxfd, listenfd, connfd, sockfd;
    char buf[BUFSIZ], str[INET_ADDRSTRLEN];         /* #define INET_ADDRSTRLEN 16 */
    struct sockaddr_in clie_addr, serv_addr;
    socklen_t clie_addr_len;
    fd_set rset, allset;                            /* rset 读事件文件描述符集合 allset用来暂存 */

    listenfd = Socket(AF_INET, SOCK_STREAM, 0);
    //端口复用
    int opt = 1;
    setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));


    bzero(&serv_addr, sizeof(serv_addr));
    serv_addr.sin_family= AF_INET;
    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    serv_addr.sin_port= htons(SERV_PORT);


    Bind(listenfd, (struct sockaddr *)&serv_addr, sizeof(serv_addr));
    Listen(listenfd, 128);

    maxfd = listenfd;                                           /* 起初 listenfd 即为最大文件描述符 */

    maxi = -1;                                                  /* 将来用作client[]的下标, 初始值指向0个元素之前下标位置 */
    for (i = 0; i < FD_SETSIZE; i++)
        client[i] = -1;                                         /* 用-1初始化client[] */

    FD_ZERO(&allset);
    FD_SET(listenfd, &allset);                                  /* 构造select监控文件描述符集 */

    while (1) {   
        rset = allset;                                          /* 每次循环时都从新设置select监控信号集 */

        nready = select(maxfd+1, &rset, NULL, NULL, NULL);      //2  1--lfd  1--connfd
        if (nready < 0)
            perr_exit("select error");

        if (FD_ISSET(listenfd, &rset)) {                        /* 说明有新的客户端链接请求 */

            clie_addr_len = sizeof(clie_addr);
            connfd = Accept(listenfd, (struct sockaddr *)&clie_addr, &clie_addr_len);       /* Accept 不会阻塞 */
            printf("received from %s at PORT %d\n",
                    inet_ntop(AF_INET, &clie_addr.sin_addr, str, sizeof(str)),
                    ntohs(clie_addr.sin_port));

            for (i = 0; i < FD_SETSIZE; i++)
                if (client[i] < 0) {                            /* 找client[]中没有使用的位置 */
                    client[i] = connfd;                         /* 保存accept返回的文件描述符到client[]里 */
                    break;
                }

            if (i == FD_SETSIZE) {                              /* 达到select能监控的文件个数上限 1024 */
                fputs("too many clients\n", stderr);
                exit(1);
            }

            FD_SET(connfd, &allset);                            /* 向监控文件描述符集合allset添加新的文件描述符connfd */

            if (connfd > maxfd)
                maxfd = connfd;                                 /* select第一个参数需要 */

            if (i > maxi)
                maxi = i;                                       /* 保证maxi存的总是client[]最后一个元素下标 */

            if (--nready == 0)
                continue;
        } 

        for (i = 0; i <= maxi; i++) {                               /* 检测哪个clients 有数据就绪 */

            if ((sockfd = client[i]) < 0)
                continue;//数组内的文件描述符如果被释放有可能变成-1
            if (FD_ISSET(sockfd, &rset)) {

                if ((n = Read(sockfd, buf, sizeof(buf))) == 0) {    /* 当client关闭链接时,服务器端也关闭对应链接 */
                    Close(sockfd);
                    FD_CLR(sockfd, &allset);                        /* 解除select对此文件描述符的监控 */
                    client[i] = -1;
                } else if (n > 0) {
                    for (j = 0; j < n; j++)
                        buf[j] = toupper(buf[j]);
                    Write(sockfd, buf, n);
                    Write(STDOUT_FILENO, buf, n);
                }
                if (--nready == 0)
                    break;                                          /* 跳出for, 但还在while中 */
            }
        }
    }

    Close(listenfd);

    return 0;
}
```

## 13.2 poll函数

> select() 和 poll() 系统调用的本质一样，poll() 的机制与 select() 类似，与 select() 在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是 poll() 没有最大文件描述符数量的限制（但是数量过大后性能也是会下降）。poll() 和 select() 同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。

```c
#include <pool.h>
int poll(struct pollfd *fd, nfds_t nfds, int timeout);

struct pollfd
{
	int fd;	//文件描述符
	short events;	//监控的事件，POLLIN 读事件 | POLLOUT 写事件
	short revent;	//监控事件中满足条件返回的事件
}

/*
	POLLIN			普通或带外优先数据可读,即POLLRDNORM | POLLRDBAND
	POLLRDNORM		数据可读
	POLLRDBAND		优先级带数据可读
	POLLPRI 		高优先级可读数据
	POLLOUT		普通或带外数据可写
	POLLWRNORM		数据可写
	POLLWRBAND		优先级带数据可写
	POLLERR 		发生错误
	POLLHUP 		发生挂起
	POLLNVAL 		描述字不是一个打开的文件
	
	nfds 			监控数组中有多少文件描述符需要被监控

	timeout 		毫秒级等待
		-1：阻塞等，#define INFTIM -1 				Linux中没有定义此宏
		0：立即返回，不阻塞进程
		>0：等待指定毫秒数，如当前系统时间精度不够毫秒，向上取值
返回值：
	成功时，poll() 返回结构体中 revents 域不为 0 的文件描述符个数；如果在超时前没有任何事件发生，poll()返回 0；

	失败时，poll() 返回 -1，并设置 errno 为下列值之一：

EBADF：一个或多个结构体中指定的文件描述符无效。
EFAULT：fds 指针指向的地址超出进程的地址空间。
EINTR：请求的事件之前产生一个信号，调用可以重新发起。
EINVAL：nfds 参数超出 PLIMIT_NOFILE 值。
ENOMEM：可用内存不足，无法完成请求。
*/
```

【例】

```c
/* server.c */
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <poll.h>
#include <errno.h>
#include "wrap.h"

#define MAXLINE 80
#define SERV_PORT 6666
#define OPEN_MAX 1024

int main(int argc, char *argv[])
{
	int i, j, maxi, listenfd, connfd, sockfd;
	int nready;
	ssize_t n;
	char buf[MAXLINE], str[INET_ADDRSTRLEN];
	socklen_t clilen;
	struct pollfd client[OPEN_MAX];
	struct sockaddr_in cliaddr, servaddr;

	listenfd = Socket(AF_INET, SOCK_STREAM, 0);

	bzero(&servaddr, sizeof(servaddr));
	servaddr.sin_family = AF_INET;
	servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
	servaddr.sin_port = htons(SERV_PORT);

	Bind(listenfd, (struct sockaddr *)&servaddr, sizeof(servaddr));

	Listen(listenfd, 20);

	client[0].fd = listenfd;
	client[0].events = POLLIN; 					/* listenfd监听普通读事件 */
	for (i = 1; i < OPEN_MAX; i++)
		client[i].fd = -1; 							/* 用-1初始化client[]里剩下元素 */
	maxi = 0; 										/* client[]数组有效元素中最大元素下标 */

	for ( ; ; ) {
		nready = poll(client, maxi+1, -1); 			/* 阻塞 */
		if (client[0].revents & POLLIN) { 		/* 有客户端链接请求 */
			clilen = sizeof(cliaddr);
			connfd = Accept(listenfd, (struct sockaddr *)&cliaddr, &clilen);
			printf("received from %s at PORT %d\n",
					inet_ntop(AF_INET, &cliaddr.sin_addr, str, sizeof(str)),
					ntohs(cliaddr.sin_port));
			for (i = 1; i < OPEN_MAX; i++) {
				if (client[i].fd < 0) {
					client[i].fd = connfd; 	/* 找到client[]中空闲的位置，存放accept返回的connfd */
					break;
				}
			}

			if (i == OPEN_MAX)
				perr_exit("too many clients");

			client[i].events = POLLIN; 		/* 设置刚刚返回的connfd，监控读事件 */
			if (i > maxi)
				maxi = i; 						/* 更新client[]中最大元素下标 */
			if (--nready <= 0)
				continue; 						/* 没有更多就绪事件时,继续回到poll阻塞 */
		}
		for (i = 1; i <= maxi; i++) { 			/* 检测client[] */
			if ((sockfd = client[i].fd) < 0)
				continue;
			if (client[i].revents & POLLIN) {
				if ((n = Read(sockfd, buf, MAXLINE)) < 0) {
					if (errno == ECONNRESET) { /* 当收到 RST标志时 */
						/* connection reset by client */
						printf("client[%d] aborted connection\n", i);
						Close(sockfd);
						client[i].fd = -1;
					} else {
						perr_exit("read error");
					}
				} else if (n == 0) {
					/* connection closed by client */
					printf("client[%d] closed connection\n", i);
					Close(sockfd);
					client[i].fd = -1;
				} else {
				for (j = 0; j < n; j++)
						buf[j] = toupper(buf[j]);
						Writen(sockfd, buf, n);
				}
				if (--nready <= 0)
					break; 				/* no more readable descriptors */
			}
		}
	}
	return 0;
}
```

```c
/* client.c */
#include <stdio.h>
#include <string.h>
#include <unistd.h>
#include <netinet/in.h>
#include "wrap.h"

#define MAXLINE 80
#define SERV_PORT 6666

int main(int argc, char *argv[])
{
	struct sockaddr_in servaddr;
	char buf[MAXLINE];
	int sockfd, n;

	sockfd = Socket(AF_INET, SOCK_STREAM, 0);

	bzero(&servaddr, sizeof(servaddr));
	servaddr.sin_family = AF_INET;
	inet_pton(AF_INET, "127.0.0.1", &servaddr.sin_addr);
	servaddr.sin_port = htons(SERV_PORT);

	Connect(sockfd, (struct sockaddr *)&servaddr, sizeof(servaddr));

	while (fgets(buf, MAXLINE, stdin) != NULL) {
		Write(sockfd, buf, strlen(buf));
		n = Read(sockfd, buf, MAXLINE);
		if (n == 0)
			printf("the other side has been closed.\n");
		else
			Write(STDOUT_FILENO, buf, n);
	}
	Close(sockfd);
	return 0;
}
```

## 13.3 epoll

> ### select的缺陷
>
>    [高并发](https://so.csdn.net/so/search?q=高并发&spm=1001.2101.3001.7020)的核心解决方案是1个线程处理所有连接的“等待消息准备好”，这一点上epoll和select是无争议的。但select预估错误了一件事，当数十万并发连接存在时，可能每一毫秒只有数百个活跃的连接，同时其余数十万连接在这一毫秒是非活跃的。select的使用方法是这样的：
>
>    返回的活跃连接 ==select（全部待监控的连接）。
>
>    什么时候会调用select方法呢？在你认为需要找出有报文到达的活跃连接时，就应该调用。所以，调用select在[高并发](https://so.csdn.net/so/search?q=高并发&spm=1001.2101.3001.7020)时是会被频繁调用的。这样，这个频繁调用的方法就很有必要看看它是否有效率，因为，它的轻微效率损失都会被“频繁”二字所放大。它有效率损失吗？显而易见，全部待监控连接是数以十万计的，返回的只是数百个活跃连接，这本身就是无效率的表现。被放大后就会发现，处理并发上万个连接时，select就完全力不从心了。
>
>    此外，在Linux内核中，select所用到的FD_SET是有限的，即内核中有个参数__FD_SETSIZE定义了每个FD_SET的句柄个数。
>
> 其次，内核中实现 select是用**轮询**方法，即每次检测都会遍历所有FD_SET中的句柄，显然，select函数执行时间与FD_SET中的句柄个数有一个比例关系，即 select要检测的句柄数越多就会越费时。看到这里，您可能要要问了，你为什么不提poll？笔者认为select与poll在内部机制方面并没有太大的差异。相比于select机制，poll只是取消了最大监控文件描述符数限制，并没有从根本上解决select存在的问题。

> 要深刻理解epoll，首先得了解epoll的三大关键要素：**mmap、红黑树、链表**。
>
>    epoll是通过内核与用户空间mmap同一块内存实现的。mmap将用户空间的一块地址和内核空间的一块地址同时映射到相同的一块物理内存地址（不管是用户空间还是内核空间都是虚拟地址，最终要通过地址映射映射到物理地址），使得这块物理内存对内核和对用户均可见，减少用户态和内核态之间的数据交换。内核可以直接看到epoll监听的句柄，效率高。
>
>    [红黑树](https://so.csdn.net/so/search?q=红黑树&spm=1001.2101.3001.7020)将存储epoll所监听的套接字。上面mmap出来的内存如何保存epoll所监听的套接字，必然也得有一套数据结构，epoll在实现上采用红黑树去存储所有套接字，当添加或者删除一个套接字时（epoll_ctl），都在红黑树上去处理，红黑树本身插入和删除性能比较好，时间复杂度O(logN)。
>
> 通过epoll_ctl函数添加进来的事件都会被放在红黑树的某个节点内，所以，重复添加是没有用的。当把事件添加进来的时候时候会完成关键的一步，那就是该事件都会与相应的设备（网卡）驱动程序建立回调关系，当相应的事件发生后，就会调用这个回调函数，该回调函数在内核中被称为：ep_poll_callback,**这个回调函数其实就所把这个事件添加到rdllist这个双向链表中**。一旦有事件发生，epoll就会将该事件添加到双向链表中。那么当我们调用epoll_wait时，epoll_wait只需要检查rdlist双向链表中是否有存在注册的事件，效率非常可观。这里也需要将发生了的事件复制到用户态内存中即可。

```c
//1 创建红黑树
#include <sys/epoll.h>

int epoll_create(int size)
/*
size: 监听的文件描述符上限，内核2.6版本后可以自动扩展，填写大于0即可
返回值：
	成功：非负文件描述符
	失败：-1，并设置相应的errno
*/
    
//2 上树和下树
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

struct epoll_event
{
    uint32_t events;	// Epoll events
    epoll_data_t data; //需要监听的文件描述符
}

typedef union epoll_data
{
    void *ptr;
    int fd;
    uint32_t u32;
    uint64_t u64;
}epoll_data_t;
/*
efd：树的文件描述符
op：EPOLL_CTL_ADD 上树		EPOLL_CTL_DEL 下树	EPOLL_CTL_MOD 修改
*/

//3 监听
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
/*
		epfd：监听树的文件描述符
		events：	用来存内核得到事件的集合，可简单看作数组。
		maxevents：	告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，
		timeout：	是超时时间
			-1：	阻塞
			0：	立即返回，非阻塞
			>0：	指定毫秒
		返回值：	成功，返回有多少文件描述符就绪，时间到时返回0，出错返回-1
*/
```

【例】

```c
//将cfd上树
int epfd = epoll_create(1);
struct epoll_event ev;
ev.data.fd = cfd;
ev.events = EPOLLIN;
epoll_ctl(epfd, EPOLL_CTL_ADD, cfd, &ev);

/*
	EPOLLIN ：	表示对应的文件描述符可以读（包括对端SOCKET正常关闭）
		EPOLLOUT：	表示对应的文件描述符可以写
		EPOLLPRI：	表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）
		EPOLLERR：	表示对应的文件描述符发生错误
		EPOLLHUP：	表示对应的文件描述符被挂断；
		EPOLLET： 	将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)而言的
		EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
*/
```

【例1】应用于进程间通信

```c
#include <stdio.h>
#include <unistd.h>
#include <string.h>
#include <sys/epoll.h>

int main(int argc, char *argv[])
{
	int fd[2];
	int ret = -1;
	pipe(fd);
	//创建子进程
	ret = fork();
	if(ret < 0)
	{
		perror("fork: ");
	}
	else if(ret == 0)
	{
		close(fd[0]);
		char buf[5];
		char ch = 'a';
		while(1)
		{
			sleep(5);
			memset(buf, ch++, sizeof(buf));
			write(fd[1], buf, sizeof(buf));
		}
	}
	else
	{
		close(fd[1]);
		//创建红黑树
		int epfd = epoll_create(1);
		struct epoll_event ev, evs[1];
		ev.data.fd = fd[0];
		ev.events = EPOLLIN;
		epoll_ctl(epfd, EPOLL_CTL_ADD, fd[0], &ev);//上树
		//监听
		while(1)
		{
			int n = epoll_wait(epfd, evs, 1, -1);
			if(n == 1)
			{
				char buf[128] = "";
				int ret = read(fd[0], buf, sizeof(buf));
				if(ret <= 0)
				{
					close(fd[0]);
					epoll_ctl(epfd, EPOLL_CTL_DEL, fd[0], &ev); //下树
					break;
				}
				else
				{
					printf("%s\n", buf);
				}
			}
		}
	}
	return 0;
}
```

【例】高并发web服务器

```c
#include <stdio.h>
#include "wrap.h"
#include <sys/epoll.h>

int main(int argc, char *argv[])
{
	//创建套接字并绑定
	int lfd = tcp4bind(8000, NULL);
	//监听
	Listen(lfd, 128);
	//创建树
	int epfd = epoll_create(1);
	//将lfd上树
	struct epoll_event ev, evs[1024];
	ev.data.fd = lfd;
	ev.events = EPOLLIN;
	epoll_ctl(epfd, EPOLL_CTL_ADD, lfd, &ev);
	//监视
	while(1)
	{
		int nready = epoll_wait(epfd, evs, 1024, -1);
		if(nready < 0)
		{
			perror("epoll_wait");
			break;
		}
		else if(nready == 0)
		{
			continue;
		}
		else
		{
			for(int i = 0; i < nready; i++)
			{
				if(evs[i].data.fd == lfd && (evs[i].events & EPOLLIN))
				{
					struct sockaddr_in cliaddr;
					char ip[16] = "";
					socklen_t len = sizeof(cliaddr);
					int cfd = Accept(lfd, (struct sockaddr *)&cliaddr, &len);
					printf("new client ip=%s port=%d\n", inet_ntop(AF_INET, &cliaddr.sin_addr, ip, 16), ntohs(cliaddr.sin_port));
					//将cfd上树
					ev.data.fd = cfd;
					ev.events = EPOLLIN;
					epoll_ctl(epfd, EPOLL_CTL_ADD, cfd, &ev);
				}
				else if(evs[i].events & EPOLLIN)
				{
					char buf[1024] = "";
					int n = read(evs[i].data.fd, buf, sizeof(buf));
					if(n < 0)
					{
						perror("read: ");
						//下树
						epoll_ctl(epfd, EPOLL_CTL_DEL, evs[i].data.fd, &evs[i]);
					}
					else if(n == 0)
					{
						printf("client close\n");
						close(evs[i].data.fd);
						epoll_ctl(epfd, EPOLL_CTL_DEL, evs[i].data.fd, &evs[i]);
					}
					else
					{
						printf("%s\n", buf);
						write(evs[i].data.fd, buf, n);
					}
				}
			}
		}
	}
	return 0;
}
```

### 13.3.1 epoll_wait的两种触发方式

> 水平触发和边沿触发：
>
> 1、监听读缓冲区的变化
>
> **水平触发（LT）：只要读缓冲区有数据就会触发epoll_wait，默认是水平触发，ev.events=EPOLLIN**
>
> **边沿触发（ET）：数据抵达一次，epoll_wait只触发一次ev.events=EPOLLIN | EPOLLET**
>
> 2、监听写缓冲区的变化
>
> 水平触发：只要可以写，就会触发
>
> 边沿触发：数据从有到无，就会触发

**ET模式**

ET模式即Edge Triggered工作模式。

如果我们在第1步将rfd添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll_wait之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。最好以下面的方式调用ET模式的epoll接口，在后面会介绍避免可能的缺陷。

1) 基于非阻塞文件句柄

2) 只有当read或者write返回EAGAIN(非阻塞读，暂时无数据)时才需要挂起、等待。但这并不是说每次read时都需要循环读，直到读到产生一个EAGAIN才认为此次事件处理完成，当read返回的读到的数据长度小于请求的数据长度时，就可以确定此时缓冲中已没有数据了，也就可以认为此事读事件已处理完成。

**LT模式**

LT模式即Level Triggered工作模式。

与ET模式不同的是，以LT方式调用epoll接口的时候，它就相当于一个速度比较快的poll，无论后面的数据是否被使用。

**比较**

LT(level triggered)：LT是**缺省**的工作方式，并且同时支持block和no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。**传统的select/poll都是这种模型的代表。**

ET(edge-triggered)：ET是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知。请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once).

【例】设置边沿触发方式，一次触发读取完所有读缓冲区数据

```c
/* ************************************************************************
 *       Filename:  02_epoll_server.c
 *    Description:  
 *        Version:  1.0
 *        Created:  2019年02月25日 12时18分53秒
 *       Revision:  none
 *       Compiler:  gcc
 *         Author:  YOUR NAME (), 
 *        Company:  
 * ************************************************************************/


#include <stdio.h>
#include <fcntl.h>
#include "wrap.h"
#include <sys/epoll.h>
int main(int argc, char *argv[])
{
	//创建套接字 绑定
	int lfd = tcp4bind(8000,NULL);
	//监听
	Listen(lfd,128);
	//创建树
	int epfd = epoll_create(1);
	//将lfd上树
	struct epoll_event ev,evs[1024];
	ev.data.fd = lfd;
	ev.events = EPOLLIN;
	epoll_ctl(epfd,EPOLL_CTL_ADD,lfd,&ev);
	//while监听
	while(1)
	{
		int nready = epoll_wait(epfd,evs,1024,-1);//监听
		printf("epoll wait _________________\n");
		if(nready <0)
		{
			perror("");
			break;
		}
		else if( nready == 0)
		{
			continue;
		}
		else//有文件描述符变化
		{
			for(int i=0;i<nready;i++)
			{
				//判断lfd变化,并且是读事件变化
				if(evs[i].data.fd == lfd && evs[i].events & EPOLLIN)
				{
					struct sockaddr_in cliaddr;
					char ip[16]="";
					socklen_t len = sizeof(cliaddr);
					int cfd = Accept(lfd,(struct sockaddr *)&cliaddr,&len);//提取新的连接

					printf("new client ip=%s port =%d\n",inet_ntop(AF_INET,&cliaddr.sin_addr.s_addr,ip,16)
							,ntohs(cliaddr.sin_port));
					//设置cfd为非阻塞
					int flags = fcntl(cfd,F_GETFL);//获取的cfd的标志位
					flags |= O_NONBLOCK;
					fcntl(cfd,F_SETFL,flags);
					//将cfd上树
					ev.data.fd =cfd;
					ev.events =EPOLLIN | EPOLLET;
					epoll_ctl(epfd,EPOLL_CTL_ADD,cfd,&ev);
				
				}
				else if( evs[i].events & EPOLLIN)//cfd变化 ,而且是读事件变化
				{
					while(1)
					{
						char buf[4]="";
						//如果读一个缓冲区,缓冲区没有数据,如果是带阻塞,就阻塞等待,如果
						//是非阻塞,返回值等于-1,并且会将errno 值设置为EAGAIN
						int n = read(evs[i].data.fd,buf,sizeof(buf));
						if(n < 0)//出错,cfd下树
						{
							//如果缓冲区读干净了,这个时候应该跳出while(1)循环,继续监听
							if(errno == EAGAIN)
							{
								break;
							
							}
							//普通错误
							perror("");
							close(evs[i].data.fd);//将cfd关闭
							epoll_ctl(epfd,EPOLL_CTL_DEL,evs[i].data.fd,&evs[i]);
							break;
						}
						else if(n == 0)//客户端关闭 ,
							{
								printf("client close\n");
								close(evs[i].data.fd);//将cfd关闭
								epoll_ctl(epfd,EPOLL_CTL_DEL,evs[i].data.fd,&evs[i]);//下树
								break;
							}
						else
						{
							//	printf("%s\n",buf);
							write(STDOUT_FILENO,buf,4);
							write(evs[i].data.fd,buf,n);
							

						}
					}

				}
			
			
			}
		
		}
	
	
	}
	
	return 0;
}
```

# 14 epoll反应堆

> epoll还有一种更高级的使用方法，那就是借鉴封装的思想，简单的说就是当某个事情发生了，自动的去处理这个事情。这样的思想对我们的编码来说就是设置回调，将文件描述符，对应的事件，和事件产生时的处理函数封装到一起，这样当某个文件描述符的事件发生了，回调函数会自动被触发，这就是所谓的反应堆思想。
>
> ​	从我们之前对epoll的使用上如何去支持反应堆呢？需要重新再认识一下struct epoll_event中的epoll_data_t结构体:
>
> ​	typedef union epoll_data {
>
> ​        void     *ptr;
>
> ​        int      fd;
>
> ​        uint32_t   u32;
>
> ​        uint64_t   u64;
>
> ​      } epoll_data_t;
>
> 我们之前使用的是共用体上的fd域,如果是要实现封装思想,光有fd是不够的,所以转换思路,看第一个域ptr,是一个泛型指针,指针可以指向一块内存区域,这块区域可以代表一个结构体,既然是结构体,那么我们就可以自定义了,将我们非常需要的文件描述符,事件类型,回调函数都封装在结构体上(我们在信号一章就见识过struct sigaction上有掩码和回调函数等信息),这样当我们要监控的文件描述符对应的事件发生之后,我们去调用回调函数就可以了，这样就可以将文件描述符对应事件的处理代码梳理的非常清晰。

```c
//反应堆简单版
#include <stdio.h>
#include <unistd.h>
#include <stdlib.h>
#include <fcntl.h>
#include <string.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <sys/epoll.h>
#include "wrap.h"

#define _BUF_LEN_  1024
#define _EVENT_SIZE_ 1024

//全局epoll树的根
int gepfd = 0;

//事件驱动结构体
typedef struct xx_event{
    int fd;
    int events;
    void (*call_back)(int fd,int events,void *arg);
    void *arg;
    char buf[1024];
    int buflen;
    int epfd;
}xevent;

xevent myevents[_EVENT_SIZE_+1];

void readData(int fd,int events,void *arg);

//添加事件
//eventadd(lfd,EPOLLIN,initAccept,&myevents[_EVENT_SIZE_-1],&myevents[_EVENT_SIZE_-1]);
void eventadd(int fd,int events,void (*call_back)(int ,int ,void *),void *arg,xevent *ev)
{
    ev->fd = fd;
    ev->events = events;
    //ev->arg = arg;//代表结构体自己,可以通过arg得到结构体的所有信息
    ev->call_back = call_back;

    struct epoll_event epv;
    epv.events = events;
    epv.data.ptr = ev;//核心思想
    epoll_ctl(gepfd,EPOLL_CTL_ADD,fd,&epv);//上树
}

//修改事件
//eventset(fd,EPOLLOUT,senddata,arg,ev);
void eventset(int fd,int events,void (*call_back)(int ,int ,void *),void *arg,xevent *ev)
{
    ev->fd = fd;
    ev->events = events;
    //ev->arg = arg;
    ev->call_back = call_back;

    struct epoll_event epv;
    epv.events = events;
    epv.data.ptr = ev;
    epoll_ctl(gepfd,EPOLL_CTL_MOD,fd,&epv);//修改
}

//删除事件
void eventdel(xevent *ev,int fd,int events)
{
	printf("begin call %s\n",__FUNCTION__);

    ev->fd = 0;
    ev->events = 0;
    ev->call_back = NULL;
    memset(ev->buf,0x00,sizeof(ev->buf));
    ev->buflen = 0;

    struct epoll_event epv;
    epv.data.ptr = NULL;
    epv.events = events;
    epoll_ctl(gepfd,EPOLL_CTL_DEL,fd,&epv);//下树
}

//发送数据
void senddata(int fd,int events,void *arg)
{
    printf("begin call %s\n",__FUNCTION__);

    xevent *ev = arg;
    Write(fd,ev->buf,ev->buflen);
    eventset(fd,EPOLLIN,readData,arg,ev);
}

//读数据
void readData(int fd,int events,void *arg)
{
    printf("begin call %s\n",__FUNCTION__);
    xevent *ev = arg;

    ev->buflen = Read(fd,ev->buf,sizeof(ev->buf));
    if(ev->buflen>0) //读到数据
	{	
		//void eventset(int fd,int events,void (*call_back)(int ,int ,void *),void *arg,xevent *ev)
        eventset(fd,EPOLLOUT,senddata,arg,ev);

    }
	else if(ev->buflen==0) //对方关闭连接
	{
        Close(fd);
        eventdel(ev,fd,EPOLLIN);
    }

}
//新连接处理
void initAccept(int fd,int events,void *arg)
{
    printf("begin call %s,gepfd =%d\n",__FUNCTION__,gepfd);//__FUNCTION__ 函数名

    int i;
    struct sockaddr_in addr;
    socklen_t len = sizeof(addr);
    int cfd = Accept(fd,(struct sockaddr*)&addr,&len);//是否会阻塞？
	
	//查找myevents数组中可用的位置
    for(i = 0 ; i < _EVENT_SIZE_; i ++)
	{
        if(myevents[i].fd==0)
		{
            break;
        }
    }

    //设置读事件
    eventadd(cfd,EPOLLIN,readData,&myevents[i],&myevents[i]);
}

int main(int argc,char *argv[])
{
	//创建socket
    int lfd = Socket(AF_INET,SOCK_STREAM,0);

    //端口复用
    int opt = 1;
    setsockopt(lfd,SOL_SOCKET,SO_REUSEADDR,&opt,sizeof(opt));

	//绑定
    struct sockaddr_in servaddr;
    servaddr.sin_family = AF_INET;
    servaddr.sin_port = htons(8888);
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    Bind(lfd,(struct sockaddr*)&servaddr,sizeof(servaddr));
    
	//监听
    Listen(lfd,128);

	//创建epoll树根节点
    gepfd = epoll_create(1024);
    printf("gepfd === %d\n",gepfd);

    struct epoll_event events[1024];

    //添加最初始事件，将侦听的描述符上树
    eventadd(lfd,EPOLLIN,initAccept,&myevents[_EVENT_SIZE_],&myevents[_EVENT_SIZE_]);
    //void eventadd(int fd,int events,void (*call_back)(int ,int ,void *),void *arg,xevent *ev)

    while(1)
	{
        int nready = epoll_wait(gepfd,events,1024,-1);
		if(nready<0) //调用epoll_wait失败
		{
			perr_exit("epoll_wait error");
			
		}
        else if(nready>0) //调用epoll_wait成功,返回有事件发生的文件描述符的个数
		{
            int i = 0;
            for(i=0;i<nready; i++)
			{
                xevent *xe = events[i].data.ptr;//取ptr指向结构体地址
                printf("fd=%d\n",xe->fd);

                if(xe->events & events[i].events)
				{
                    xe->call_back(xe->fd,xe->events,xe);//调用事件对应的回调
                }
            }
        }
    }

	//关闭监听文件描述符
	Close(lfd);

    return 0;
}
```

# 15 tcpdump命令

tcpdump 是一款强大的网络抓包工具，它使用 libpcap 库来抓取网络数据包，这个库在几乎在所有的 Linux/Unix 中都有。熟悉 tcpdump 的使用能够帮助你分析调试网络数据，本文将通过一个个具体的示例来介绍它在不同场景下的使用方法。不管你是系统管理员，程序员，云原生工程师还是 yaml 工程师，掌握 tcpdump 的使用都能让你如虎添翼，升职加薪。

## 1. 基本语法和使用方法

tcpdump 的常用参数如下：

> -A 以ASCII格式打印出所有分组，并将链路层的头最小化。
>
> -c 在收到指定的数量的分组后，tcpdump就会停止。
>
> -C 在将一个原始分组写入文件之前，检查文件当前的大小是否超过了参数file_size 中指定的大小。如果超过了指定大小，则关闭当前文件，然后在打开一个新的文件。参数 file_size 的单位是兆字节（是1,000,000字节，而不是1,048,576字节）。
>
> -d 将匹配信息包的代码以人们能够理解的汇编格式给出。
>
> -dd 将匹配信息包的代码以c语言程序段的格式给出。
>
> -ddd 将匹配信息包的代码以十进制的形式给出。
>
> -D 打印出系统中所有可以用tcpdump截包的网络接口。
>
> -e 在输出行打印出数据链路层的头部信息。
>
> -E 用spi@ipaddr algo:secret解密那些以addr作为地址，并且包含了安全参数索引值spi的IPsec ESP分组。
>
> -f 将外部的Internet地址以数字的形式打印出来。
>
> -F 从指定的文件中读取表达式，忽略命令行中给出的表达式。
>
> -i 指定监听的网络接口。
>
> -l 使标准输出变为缓冲行形式，可以把数据导出到文件。
>
> -L 列出网络接口的已知数据链路。
>
> -m 从文件module中导入SMI MIB模块定义。该参数可以被使用多次，以导入多个MIB模块。
>
> -M 如果tcp报文中存在TCP-MD5选项，则需要用secret作为共享的验证码用于验证TCP-MD5选选项摘要（详情可参考RFC 2385）。
>
> -b 在数据-链路层上选择协议，包括ip、arp、rarp、ipx都是这一层的。
>
> -n 不把网络地址转换成名字。
>
> -nn 不进行端口名称的转换。
>
> -N 不输出主机名中的域名部分。例如，‘nic.ddn.mil‘只输出’nic‘。
>
> -t 在输出的每一行不打印时间戳。
>
> -O 不运行分组分组匹配（packet-matching）代码优化程序。
>
> -P 不将网络接口设置成混杂模式。
>
> -q 快速输出。只输出较少的协议信息。
>
> -r 从指定的文件中读取包(这些包一般通过-w选项产生)。
>
> -S 将tcp的序列号以绝对值形式输出，而不是相对值。
>
> -s 从每个分组中读取最开始的snaplen个字节，而不是默认的68个字节。
>
> -T 将监听到的包直接解释为指定的类型的报文，常见的类型有rpc远程过程调用）和snmp（简单网络管理协议；）。
>
> -t 不在每一行中输出时间戳。
>
> -tt 在每一行中输出非格式化的时间戳。
>
> -ttt 输出本行和前面一行之间的时间差。
>
> -tttt 在每一行中输出由date处理的默认格式的时间戳。
>
> -u 输出未解码的NFS句柄。
>
> -v 输出一个稍微详细的信息，例如在ip包中可以包括ttl和服务类型的信息。
>
> -vv 输出详细的报文信息。
>
> -w 直接将分组写入文件中，而不是不分析并打印出来。 

```bash
$ tcpdump -i eth0 -nn -s0 -v port 80
```

- **-i** : 选择要捕获的接口，通常是以太网卡或无线网卡，也可以是 vlan 或其他特殊接口。如果该系统上只有一个网络接口，则无需指定。
- **-nn** : 单个 n 表示不解析域名，直接显示 IP；两个 n 表示不解析域名和端口。这样不仅方便查看 IP 和端口号，而且在抓取大量数据时非常高效，因为域名解析会降低抓取速度。
- **-s0** : tcpdump 默认只会截取前 96 字节的内容，要想截取所有的报文内容，可以使用 -s number， number 就是你要截取的报文字节数，如果是 0 的话，表示截取报文全部内容。
- **-v** : 使用 -v，-vv 和 -vvv 来显示更多的详细信息，通常会显示更多与特定协议相关的信息。
- port 80 : 这是一个常见的端口过滤器，表示仅抓取 80 端口上的流量，通常是 HTTP。

额外再介绍几个常用参数：

- **-p** : 不让网络接口进入混杂模式。默认情况下使用 tcpdump 抓包时，会让网络接口进入混杂模式。一般计算机网卡都工作在非混杂模式下，此时网卡只接受来自网络端口的目的地址指向自己的数据。当网卡工作在混杂模式下时，网卡将来自接口的所有数据都捕获并交给相应的驱动程序。如果设备接入的交换机开启了混杂模式，使用 -p 选项可以有效地过滤噪声。
- **-e** : 显示数据链路层信息。默认情况下 tcpdump 不会显示数据链路层信息，使用 -e 选项可以显示源和目的 MAC 地址，以及 VLAN tag 信息。例如：

```bash
$ tcpdump -n -e -c 5 not ip6

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on br-lan, link-type EN10MB (Ethernet), capture size 262144 bytes
18:27:53.619865 24:5e:be:0c:17:af > 00:e2:69:23:d3:3b, ethertype IPv4 (0x0800), length 1162: 192.168.100.20.51410 > 180.176.26.193.58695: Flags [.], seq 2045333376:2045334484, ack 3398690514, win 751, length 1108
18:27:53.626490 00:e2:69:23:d3:3b > 24:5e:be:0c:17:af, ethertype IPv4 (0x0800), length 68: 220.173.179.66.36017 > 192.168.100.20.51410: UDP, length 26
18:27:53.626893 24:5e:be:0c:17:af > 00:e2:69:23:d3:3b, ethertype IPv4 (0x0800), length 1444: 192.168.100.20.51410 > 220.173.179.66.36017: UDP, length 1402
18:27:53.628837 00:e2:69:23:d3:3b > 24:5e:be:0c:17:af, ethertype IPv4 (0x0800), length 1324: 46.97.169.182.6881 > 192.168.100.20.59145: Flags [P.], seq 3058450381:3058451651, ack 14349180, win 502, length 1270
18:27:53.629096 24:5e:be:0c:17:af > 00:e2:69:23:d3:3b, ethertype IPv4 (0x0800), length 54: 192.168.100.20.59145 > 192.168.100.1.12345: Flags [.], ack 3058451651, win 6350, length 0
5 packets captured
```

### 显示 ASCII 字符串

-A 表示使用 ASCII 字符串打印报文的全部数据，这样可以使读取更加简单，方便使用 grep 等工具解析输出内容。-X 表示同时使用十六进制和 ASCII 字符串打印报文的全部数据。这两个参数不能一起使用。例如：

```bash
$ tcpdump -A -s0 port 80
```

### 抓取特定协议的数据

后面可以跟上协议名称来过滤特定协议的流量，以 UDP 为例，可以加上参数 udp 或 protocol 17，这两个命令意思相同。

```bash
$ tcpdump -i eth0 udp
$ tcpdump -i eth0 proto 17
```

同理，tcp 与 protocol 6 意思相同。

### 抓取特定主机的数据

使用过滤器 host 可以抓取特定目的地和源 IP 地址的流量。

```bash
$ tcpdump -i eth0 host 10.10.1.1
```

也可以使用 src 或 dst 只抓取源或目的地：

```bash
$ tcpdump -i eth0 dst 10.10.1.20
```

### 将抓取的数据写入文件

使用 tcpdump 截取数据报文的时候，默认会打印到屏幕的默认输出，你会看到按照顺序和格式，很多的数据一行行快速闪过，根本来不及看清楚所有的内容。不过，tcpdump 提供了把截取的数据保存到文件的功能，以便后面使用其他图形工具（比如 wireshark，Snort）来分析。

-w 选项用来把数据报文输出到文件：

```bash
$ tcpdump -i eth0 -s0 -w test.pcap
```

### 行缓冲模式

如果想实时将抓取到的数据通过管道传递给其他工具来处理，需要使用 -l 选项来开启行缓冲模式（或使用 -c 选项来开启数据包缓冲模式）。使用 -l 选项可以将输出通过立即发送给其他命令，其他命令会立即响应。

```bash
$ tcpdump -i eth0 -s0 -l port 80 | grep 'Server:'
```

### 组合过滤器

过滤的真正强大之处在于你可以随意组合它们，而连接它们的逻辑就是常用的 与/AND/&& 、 或/OR/|| 和 非/not/!。

```bash
and or &&
or or ||
not or !
```

## 2. 过滤器

关于 tcpdump 的过滤器，这里有必要单独介绍一下。

机器上的网络报文数量异常的多，很多时候我们只关系和具体问题有关的数据报（比如访问某个网站的数据，或者 icmp 超时的报文等等），而这些数据只占到很小的一部分。把所有的数据截取下来，从里面找到想要的信息无疑是一件很费时费力的工作。而 tcpdump 提供了灵活的语法可以精确地截取关心的数据报，简化分析的工作量。这些选择数据包的语句就是过滤器（filter）！

### Host 过滤器

Host 过滤器用来过滤某个主机的数据报文。例如：

```bash
$ tcpdump host 1.2.3.4
```

该命令会抓取所有发往主机 1.2.3.4 或者从主机 1.2.3.4 发出的流量。如果想只抓取从该主机发出的流量，可以使用下面的命令：

```bash
$ tcpdump src host 1.2.3.4
```

### Network 过滤器

Network 过滤器用来过滤某个网段的数据，使用的是 CIDR 模式。可以使用四元组（x.x.x.x）、三元组（x.x.x）、二元组（x.x）和一元组（x）。四元组就是指定某个主机，三元组表示子网掩码为 255.255.255.0，二元组表示子网掩码为 255.255.0.0，一元组表示子网掩码为 255.0.0.0。例如，

抓取所有发往网段 192.168.1.x 或从网段 192.168.1.x 发出的流量：

```bash
$ tcpdump net 192.168.1
```

抓取所有发往网段 10.x.x.x 或从网段 10.x.x.x 发出的流量：

```bash
$ tcpdump net 10
```

和 Host 过滤器一样，这里也可以指定源和目的：

```bash
$ tcpdump src net 10
```

也可以使用 CIDR 格式：

```bash
$ tcpdump src net 172.16.0.0/12
```

### Proto 过滤器

Proto 过滤器用来过滤某个协议的数据，关键字为 proto，可省略。proto 后面可以跟上协议号或协议名称，支持 icmp, igmp, igrp, pim, ah, esp, carp, vrrp, udp和 tcp。因为通常的协议名称是保留字段，所以在于 proto 指令一起使用时，必须根据 shell 类型使用一个或两个反斜杠（/）来转义。Linux 中的 shell 需要使用两个反斜杠来转义，MacOS 只需要一个。

例如，抓取 icmp 协议的报文：

```bash
$ tcpdump -n proto \\icmp
# 或者
$ tcpdump -n icmp
```

### Port 过滤器

Port 过滤器用来过滤通过某个端口的数据报文，关键字为 port。例如：

```bash
$ tcpdump port 389
```

# 16 host命令

01. **命令概述**
host命令是常用的分析域名查询工具，可以用来测试域名系统工作是否正常。

host命令是一个用于执行DNS查找的简单实用程序。它通常用于将名称转换为IP地址，反之亦然。 如果没有给出参数或选项，host将打印其命令行参数和选项的简短摘要。

2. **命令格式**

   > 格式：host [选项] [参数] 

03. **常用选项**

```txt
-a	显示详细的DNS信息
-c	指定查询类型，默认值为“IN”
-C	查询指定主机的完整的SOA记录
-r	不使用递归的查询方式查询域名
-t	指定查询的域名信息类型
-v	显示指令执行的详细信息
-w	如果域名服务器没有给出应答信息，则总是等待，直到域名服务器给出应答
-W	指定域名查询的最长时间，如果在指定时间内域名服务器没有给出应答信息则退出
-4	使用IPv4查询传输 （默认）
-6	使用IPv6查询传输
```

4. **参考示例**
   4.1 查询域名对应的IP地址

   ```shell
   [deng@localhost ~]$ host www.baidu.com
   www.baidu.com is an alias for www.a.shifen.com.
   www.a.shifen.com has address 183.232.231.172
   www.a.shifen.com has address 183.232.231.174
   [deng@localhost ~]$ 
   ```

   4.2 显示执行域名查询的详细信息

   ```shell
   [deng@localhost ~]$ host -v www.baidu.com
   Trying "www.baidu.com"
   ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 59733
   ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0
   
   ;; QUESTION SECTION:
   ;www.baidu.com.                 IN      A
   ```

   4.3 查询域名的MX信息

   ```shell
   [deng@localhost ~]$ host -t MX www.baidu.com
   www.baidu.com is an alias for www.a.shifen.com.
   [deng@localhost ~]$ 
   ```

   4.4 显示详细的DNS信息

   ```shell
   [deng@localhost ~]$ host -a www.baidu.com
   Trying "www.baidu.com"
   ;; Question section mismatch: got paypal.com/ANY/IN
   ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 43952
   ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0
   
   ;; QUESTION SECTION:
   ;www.baidu.com.                 IN      ANY
   
   ;; ANSWER SECTION:
   www.baidu.com.          1200    IN      CNAME   www.a.shifen.com.
   
   Received 58 bytes from 182.254.116.116#53 in 137 ms
   [deng@localhost ~]$ 
   ```

   4.5 用谷歌的DNS（8.8.8.8）来查百度主机的IP

   ```shell
   [deng@localhost ~]$ host www.baidu.com 8.8.8.8
   Using domain server:
   Name: 8.8.8.8
   Address: 8.8.8.8#53
   Aliases: 
   
   www.baidu.com is an alias for www.a.shifen.com.
   www.a.shifen.com has address 183.232.231.174
   www.a.shifen.com has address 183.232.231.172
   [deng@localhost ~]$ 
   ```

# 17 arp命令

> ap指令用来管理系统的arp缓冲区，可以显示、删除、添加静态mac地址。ARP以各种方式操纵内核的ARP缓存。主要选项是清除地址映射项并手动设置。为了调试目的，ARP程序还允许对ARP缓存进行完全转储。

# 18 iptables命令

> iptables命令是Linux上常用的防火墙软件，是netfilter项目的一部分。可以直接配置，也可以通过许多前端和图形界面配置。它可以用来过滤、阻塞不需要的流量，允许正常的网络流浪通行。

## 18.1 语法

```bash
iptables -t 表名 <-A/I/D/R> 规则链名 [规则号] <-i/o 网卡名> -p 协议名 <-s 源IP/源子网> --sport 源端口 <-d 目标IP/目标子网> --dport 目标端口 -j 动作
```

## 18.2 选项

> -t<表>：指定要操纵的表；
>
> -A：向规则链中添加条目；
>
> -D：从规则链中删除条目；
>
> -i：向规则链中插入条目；
>
> -R：替换规则链中的条目；
>
> -L：显示规则链中已有的条目；
>
> -F：清楚规则链中已有的条目；
>
> -Z：清空规则链中的数据包计算器和字节计数器；
>
> -N：创建新的用户自定义规则链；
>
> -P：定义规则链中的默认目标；
>
> -h：显示帮助信息；
>
> -p：指定要匹配的数据包协议类型；
>
> -s：指定要匹配的数据包源ip地址；
>
> -j<目标>：指定要跳转的目标；
>
> -i<网络接口>：指定数据包进入本机的网络接口；
>
> -o<网络接口>：指定数据包要离开本机所使用的网络接口。

## 18.3 命令解释

**1、表名**

- raw：高级功能，如：网址过滤。
- mangle：数据包修改（QOS），用于实现服务质量。
- net：地址转换，用于网关路由器。
- filter：包过滤，用于防火墙规则。

**2、规则链名**

- INPUT链：处理输入数据包。
- OUTPUT链：处理输出数据包。
- PORWARD链：处理转发数据包。
- PREROUTING链：用于目标地址转换（DNAT）。
- POSTOUTING链：用于源地址转换（SNAT）。

**3、动作**

- accept：接收数据包。
- drop：丢弃数据包。
- redirct：重定向、映射、透明代理。
- snat：源地址转换。
- dnat：目标地址转换。
- masquerade：IP伪装（NAT），用于ADSL。
- log：日志记录。

## 18.4 常用命令

**1、开放指定的端口**



```bash
iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT               #允许本地回环接口(即运行本机访问本机)
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT    #允许已建立的或相关连的通行
iptables -A OUTPUT -j ACCEPT         #允许所有本机向外的访问
iptables -A INPUT -p tcp --dport 22 -j ACCEPT    #允许访问22端口
iptables -A INPUT -p tcp --dport 80 -j ACCEPT    #允许访问80端口
iptables -A INPUT -p tcp --dport 21 -j ACCEPT    #允许ftp服务的21端口
iptables -A INPUT -p tcp --dport 20 -j ACCEPT    #允许FTP服务的20端口
iptables -A INPUT -j reject       #禁止其他未允许的规则访问
iptables -A FORWARD -j REJECT     #禁止其他未允许的规则访问
```

**2、屏蔽IP**



```bash
iptables -I INPUT -s 123.45.6.7 -j DROP       #屏蔽单个IP的命令
iptables -I INPUT -s 123.0.0.0/8 -j DROP      #封整个段即从123.0.0.1到123.255.255.254的命令
iptables -I INPUT -s 124.45.0.0/16 -j DROP    #封IP段即从123.45.0.1到123.45.255.254的命令
iptables -I INPUT -s 123.45.6.0/24 -j DROP    #封IP段即从123.45.6.1到123.45.6.254的命令是
```

**3、查看已添加的iptables规则**

iptables -L -n -v



```bash
Chain INPUT (policy DROP 48106 packets, 2690K bytes)
 pkts bytes target     prot opt in     out     source               destination         
 5075  589K ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0           
 191K   90M ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:22
1499K  133M ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:80
4364K 6351M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED
 6256  327K ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes)
 pkts bytes target     prot opt in     out     source               destination         
 5075  589K ACCEPT     all  --  *      lo      0.0.0.0/0            0.0.0.0/0  
```

**4、删除已添加的iptables规则**

将所有iptables以序号标记显示，执行：



```undefined
iptables -L -n --line-numbers
```

比如要删除INPUT里序号为8的规则，执行：



```bash
iptables -D INPUT 8
```

# 网络内核变量文件汇总

> 1. /proc/sys/net/ipv4/tcp_syn_retries 定义了tcp发送SYN报文后，超时重传的次数（包括第一次，默认为6）
>
> 2. /proc/sys/ipv4/tcp_timestamps 定义了是否开启tcp头部选项中的时间戳，该选项用于计算通信双方的回路时间（RTT）
>
> 3. /proc/sys/ipv4/tcp_sack 定义了是否启用选择性确认选项，启用后，tcp报文段超时重传时只需要重新发送丢失的报文段，而不需要重新发送丢失报文段后续的已发送的报文段，一般默认开启。
>
> 4. /proc/sys/net/ipv4/tcp_window_scling 定义了tcp连接的窗口扩大因子，假设窗口扩大因子为M（左移位数），tcp头部中的接收通告窗口大小为N，那么tcp报文段的实际接收通告窗口大小为N * 2 ^ M（N << M）
>
> 5. /proc/sys/net/ipv4/conf/all/accept_redirects 指定是否允许接收ICMP重定向报文。一般来说，主机只能接收ICMP重定向报文，而路由器只能发送ICMP重定向报文。
>
> 6. /proc/sys/net/ipv4/conf/all/accept_redirects 指定是否允许发送ICMP重定向报文
>
> 7. /proc/sys/net/ipv4/ip_forward 指定主机是否具有数据报转发功能。
>
> 8. /etc/protocols 定义了所有上层协议对应的protocol字段的数值。其中，ICMP是1，TCP是6，UDP是17
>
> 9. /etc/services 可查看所有知名的应用层协议，以及它们能使用哪些传输层服务。
>
> 10. /proc/sys/net/ipv4/tcp_max_orphans 和 /proc/sys/net/ipv4/tcp_fin_timeout 指定了内核能接管的孤儿连接数目，后者指定孤儿连接在内核中生存的时间。客户端发送结束报文段后进入FIN_WAIT_1状态，收到服务端ACK后进入FIN_WAIT_2状态，等待来自服务端的结束报文段，但此时客户端进程未等服务端关闭连接就强行退出，该连接将由内核来接管，称之为孤儿连接。
>
> 11. 、/proc/sys/net/ipv4/tcp_congestion_control 指定了当前机器所使用的拥塞控制算法，如reno算法，cubic算法和vegas算法等。
>
> 12. /etc/resolv.conf 存储了DNS服务器的IP地址
>
> 13. 环境变量：http_proxy指定了访问web服务时使用的代理服务器
>
> 14. /etc/hosts 指定了本地局域网内主机名与其IP的映射关系
>
> 15. /etc/host.conf 指定了系统解析主机名的方法和顺序
>
>     > ```shell
>     > order hosts,bind
>     > multi on
>     > ```
>     >
>     > 第一行表示优先访问本地/etc/hosts，失败后再使用DNS服务(bind)
>     >
>     > 第二行表示如果/etc/host文件一个主机名对应多个IP地址，那么解析结果就包含多个IP地址。
>
> 16. /proc/sys/net/ipv4/tcp_max_syn_backlog定义了某个服务端socket处于半连接状态的socket的上限。



